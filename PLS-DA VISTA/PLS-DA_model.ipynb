{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, accuracy_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"mCH_on_off_calc_params.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Separate features (X) and experimental values (Y)\n",
    "X = data.iloc[:, 5:]  # Features: all columns after the first five\n",
    "Y = data.iloc[:, :5]  # Targets: first five columns\n",
    "\n",
    "# Target variables for classification\n",
    "target_columns = [\"OFF AVG\", \"ON AVG Truncated\", \"ON AVG Full\", \"ON OFF Truncated\", \"ON OFF Full\"]\n",
    "\n",
    "# Dictionary to store model parameters\n",
    "model_params = {}\n",
    "\n",
    "# Dictionary to collect all data to export to Excel\n",
    "excel_data = {}\n",
    "\n",
    "# Initialize subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=len(target_columns), figsize=(5 * len(target_columns), 15))\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "# Loop through each target variable\n",
    "for i, target in enumerate(target_columns):\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Define class thresholds\n",
    "    threshold_high = Y[target].quantile(0.65)  \n",
    "\n",
    "    # Create binary labels: \"High\" for values above the 65th percentile, all others will be classified as \"Low\"\n",
    "    Y_classes = pd.cut(Y[target], bins=[-float(\"inf\"), threshold_high, float(\"inf\")], labels=[\"Low\", \"High\"])\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_encoded = label_encoder.fit_transform(Y_classes)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Recursive Feature Elimination (RFE) with Logistic Regression\n",
    "    estimator = LogisticRegression()\n",
    "    selector = RFE(estimator, n_features_to_select=20)  \n",
    "    X_train_rfe = selector.fit_transform(X_train, Y_train)\n",
    "    X_test_rfe = selector.transform(X_test)\n",
    "\n",
    "    selected_feature_indices = selector.support_\n",
    "    selected_feature_names = X.columns[selected_feature_indices].tolist()\n",
    "\n",
    "    # Perform cross-validation to determine the optimal number of components\n",
    "    n_components_range = range(1, min(21, X_train_rfe.shape[1] + 1))\n",
    "    scores = []\n",
    "\n",
    "    for n in n_components_range:\n",
    "        pls = PLSRegression(n_components=n)\n",
    "        X_latent = pls.fit_transform(X_train_rfe, Y_train)[0]\n",
    "        classifier = LogisticRegression()\n",
    "        score = cross_val_score(classifier, X_latent, Y_train, cv=5, scoring='accuracy').mean()\n",
    "        scores.append(score)\n",
    "        best_cv_accuracy = max(scores)\n",
    "        cv_error = 1 - best_cv_accuracy\n",
    "\n",
    "    # Get the best number of components\n",
    "    best_n_components = n_components_range[np.argmax(scores)]\n",
    "\n",
    "    plt.rcParams['font.family'] = 'Arial'  \n",
    "    plt.rcParams['font.size'] = 12  \n",
    "\n",
    "    # Plot cross-validation accuracy vs. number of components\n",
    "    axes[0, i].plot(n_components_range, scores, marker='o', linestyle='-')\n",
    "    axes[0, i].set_title(f\"Cross-Validation Accuracy: {target}\")\n",
    "    axes[0, i].set_xlabel(\"Number of Components\")\n",
    "    axes[0, i].set_ylabel(\"Accuracy\")\n",
    "    axes[0, i].grid()\n",
    "\n",
    "    # Fit the final PLS model with optimal components\n",
    "    pls = PLSRegression(n_components=best_n_components)\n",
    "    X_train_latent = pls.fit_transform(X_train_rfe, Y_train)[0]\n",
    "    X_test_latent = pls.transform(X_test_rfe)\n",
    "\n",
    "    # Train logistic regression classifier on latent variables\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train_latent, Y_train)\n",
    "    Y_pred = classifier.predict(X_test_latent)\n",
    "\n",
    "    # Scatter plot of PLS-DA latent variables\n",
    "    scatter = axes[1, i].scatter(X_train_latent[:, 0], X_train_latent[:, 1], c=Y_train, cmap='bwr', edgecolor='k')\n",
    "    axes[1, i].set_title(f\"PLS-DA Latent Variables: {target}\")\n",
    "    axes[1, i].set_xlabel(\"Latent Variable 1\")\n",
    "    axes[1, i].set_ylabel(\"Latent Variable 2\")\n",
    "    plt.colorbar(scatter, ax=axes[1, i])\n",
    "\n",
    "    # Get PLS regression loadings\n",
    "    loadings = pls.x_loadings_\n",
    "\n",
    "    # Create DataFrame for visualization\n",
    "    feature_names = np.array(X.columns)[selector.support_]\n",
    "    loadings_df = pd.DataFrame(loadings, index=feature_names, columns=[f\"LV{i+1}\" for i in range(best_n_components)])\n",
    "\n",
    "    # Plot feature importance (loadings) for the first latent variable\n",
    "    loadings_df.iloc[:, 0].sort_values().plot(kind='barh', ax=axes[2, i], color='blue')\n",
    "    axes[2, i].set_title(f\"Feature Importance: {target}\")\n",
    "    axes[2, i].set_xlabel(\"Loading Value\")\n",
    "    axes[2, i].set_ylabel(\"Feature\")\n",
    "\n",
    "    # Compute accuracy and classification metrics\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "    class_report = classification_report(Y_test, Y_pred, target_names=label_encoder.classes_)\n",
    "\n",
    "    print(f\"Accuracy for {target}: {accuracy}\")\n",
    "    print(f\"Confusion Matrix for {target}:\\n{conf_matrix}\")\n",
    "    print(f\"Classification Report for {target}:\\n{class_report}\")\n",
    "\n",
    "    # Save model parameters\n",
    "    model_params[target] = {\n",
    "        \"pls_loadings\": pls.x_loadings_,\n",
    "        \"logreg_coefficients\": classifier.coef_,\n",
    "        \"scaler_mean\": scaler.mean_,\n",
    "        \"scaler_scale\": scaler.scale_,\n",
    "        \"selected_feature_indices\": selected_feature_indices,  # Save selected feature indices\n",
    "        \"selected_feature_names\": selected_feature_names,     # Save selected feature names\n",
    "    }\n",
    "    \n",
    "    # Compute ROC Curve & AUC\n",
    "    Y_prob = classifier.predict_proba(X_test_latent)[:, 1]  # Probabilities for \"High\" class\n",
    "    fpr, tpr, _ = roc_curve(Y_test, Y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(roc_auc)\n",
    "\n",
    "    roc_data = pd.DataFrame({\n",
    "    'False Positive Rate': fpr,\n",
    "    'True Positive Rate': tpr})\n",
    "    print(roc_data)\n",
    "\n",
    "    # Plot ROC Curve (Top Row)\n",
    "    ax1 = axes[3, i]\n",
    "    ax1.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "    ax1.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random guess line\n",
    "    ax1.set_title(f'ROC Curve ({target})')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# Save all parameters to a file\n",
    "with open(\"all_trained_model_params.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_params, f)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pls_DA_accuracy_loading_feature.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
